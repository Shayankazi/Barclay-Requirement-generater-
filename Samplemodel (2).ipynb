{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UohH29rogadX"
      },
      "outputs": [],
      "source": [
        "!pip install PyPDF2\n",
        "import PyPDF2\n",
        "import re\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    with open(pdf_path, 'rb') as file:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "        text = \"\"\n",
        "        for page in reader.pages:\n",
        "            text += page.extract_text()\n",
        "    # Replace all newlines with spaces\n",
        "    text = text.replace('\\n', ' ')\n",
        "    # Use regex to add newlines after sentence endings\n",
        "    text = re.sub(r'(?<=[.?!]) +', '\\n', text)\n",
        "    return text\n",
        "\n",
        "# Install the python-docx library\n",
        "!pip install python-docx # This line installs the necessary library\n",
        "\n",
        "# Extracting text from Word documents using python-docx\n",
        "from docx import Document\n",
        "\n",
        "def extract_text_from_docx(docx_path):\n",
        "    doc = Document(docx_path)\n",
        "    text = \"\"\n",
        "    for para in doc.paragraphs:\n",
        "        text += para.text\n",
        "        if not para.text.endswith(\"\\n\"):\n",
        "            text += \"\\n\"\n",
        "    return text\n",
        "\n",
        "extracted_text = extract_text_from_docx(\"Problem Statement.docx\")\n",
        "print(extracted_text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download stopwords if not already downloaded\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove non-alphanumeric characters (except for spaces)\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "\n",
        "    # Tokenize the text into words\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_words = [word for word in words if word not in stop_words]\n",
        "\n",
        "    # Join the words back into a string\n",
        "    clean_text = ' '.join(filtered_words)\n",
        "\n",
        "    return clean_text\n",
        "\n",
        "# Example usage\n",
        "text_from_pdf = extract_text_from_pdf('Problem Statement.pdf')\n",
        "clean_text = preprocess_text(text_from_pdf)\n",
        "print(clean_text)\n"
      ],
      "metadata": {
        "id": "GJsRxhG0tb3L",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "!pip install rake_nltk\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "from rake_nltk import Rake  # For keyword extraction\n",
        "import spacy  # For dependency parsing\n",
        "import PyPDF2\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Function to extract text from PDF\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    with open(pdf_path, 'rb') as file:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "        text = \"\"\n",
        "        for page in reader.pages:\n",
        "            text += page.extract_text() or \"\"\n",
        "    return text\n",
        "\n",
        "# Function to preprocess text\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "    words = word_tokenize(text)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_words = [word for word in words if word not in stop_words]\n",
        "    clean_text = ' '.join(filtered_words)\n",
        "    return clean_text\n",
        "\n",
        "# Function to extract user requirements\n",
        "def extract_user_requirements(text):\n",
        "    # Keyword Extraction using RAKE\n",
        "    r = Rake()\n",
        "    r.extract_keywords_from_text(text)\n",
        "    keywords = r.get_ranked_phrases_with_scores()\n",
        "\n",
        "    # Filter keywords related to user needs (e.g., \"need,\" \"require,\" \"want\")\n",
        "    user_requirement_keywords = [phrase for score, phrase in keywords if any(word in phrase for word in [\"need\", \"require\", \"want\"])]\n",
        "\n",
        "    # Dependency Parsing using spaCy\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Extract sentences containing user requirement keywords\n",
        "    user_requirement_sentences = []\n",
        "    for sent in doc.sents:\n",
        "        if any(keyword in sent.text for keyword in user_requirement_keywords):\n",
        "            user_requirement_sentences.append(sent.text)\n",
        "\n",
        "    return user_requirement_sentences\n",
        "\n",
        "# Upload file in Google Colab if needed\n",
        "# from google.colab import files\n",
        "# uploaded = files.upload()\n",
        "\n",
        "# Example usage\n",
        "pdf_path = 'PS2.pdf'  # Make sure the file exists\n",
        "text_from_pdf = extract_text_from_pdf(pdf_path)\n",
        "clean_text = preprocess_text(text_from_pdf)\n",
        "user_requirements = extract_user_requirements(clean_text)\n",
        "\n",
        "print(user_requirements)\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "7YvQh-nuvpGX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy"
      ],
      "metadata": {
        "id": "akFQUav7v2px"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load Google's abstractive summarization model\n",
        "summarization_pipeline = pipeline(\"summarization\", model=\"google/pegasus-xsum\")\n",
        "\n",
        "def generate_requirement_statement(text):\n",
        "    \"\"\"\n",
        "    Generate structured user requirement statements using abstractive summarization.\n",
        "    \"\"\"\n",
        "    summarized_text = summarization_pipeline(text, max_length=70, min_length=10, do_sample=False)\n",
        "    return summarized_text[0]['summary_text']\n",
        "\n",
        "# Example usage\n",
        "pdf_path = 'Problem Statement.pdf'  # Make sure the file exists\n",
        "text_from_pdf = extract_text_from_pdf(pdf_path)\n",
        "clean_text = preprocess_text(text_from_pdf)\n",
        "\n",
        "# Generate structured requirement statement\n",
        "requirement_statement = generate_requirement_statement(clean_text)\n",
        "print(\"Generated Requirement Statement:\", requirement_statement)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGABJCtX34jK",
        "outputId": "c1acbe57-0999-4088-a461-215f5aee7201"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-xsum and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Requirement Statement: Hospital management app allows patients to book appointments online doctors based availability app secure system storing patient records prescribe medicines digitally separate section staff Hospital management app allows patients to book appointments online doctors based availability app secure system storing patient records prescribe medicines digitally separate section staff Hospital management app allows patients to book appointments online doctors based availability app secure system storing patient records prescribe medicines\n"
          ]
        }
      ]
    }
  ]
}